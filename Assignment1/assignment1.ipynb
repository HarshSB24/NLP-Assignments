{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6cb3883",
   "metadata": {},
   "source": [
    "# NLP Assignment 1: Tokenization, Stemming, and Lemmatization\n",
    "\n",
    "This notebook demonstrates various NLP techniques using NLTK:\n",
    "- **Tokenization**: Whitespace, Punctuation-based, Treebank, Tweet, and MWE (Multi-Word Expression)\n",
    "- **Stemming**: Porter Stemmer and Snowball Stemmer\n",
    "- **Lemmatization**: WordNet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbef8b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported and data downloaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer, WordPunctTokenizer, TreebankWordTokenizer, TweetTokenizer, MWETokenizer\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "print(\"All libraries imported and data downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af4a8ac",
   "metadata": {},
   "source": [
    "## Sample Text\n",
    "\n",
    "Let's define sample texts for demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6693b672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text:\n",
      "Hello, world! This is a sample text for NLP. It contains words like running, flies, better, and easily.\n",
      "\n",
      "Tweet Text:\n",
      "@NLPStudent: Just learned about #NLP and #MachineLearning! ðŸ˜Š Check it out: https://example.com\n"
     ]
    }
   ],
   "source": [
    "# Sample text for general tokenization\n",
    "text = \"Hello, world! This is a sample text for NLP. It contains words like running, flies, better, and easily.\"\n",
    "\n",
    "# Sample text for tweet tokenization\n",
    "tweet_text = \"@NLPStudent: Just learned about #NLP and #MachineLearning! ðŸ˜Š Check it out: https://example.com\"\n",
    "\n",
    "print(\"Sample Text:\")\n",
    "print(text)\n",
    "print(\"\\nTweet Text:\")\n",
    "print(tweet_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2261045f",
   "metadata": {},
   "source": [
    "## 1. Tokenization Techniques\n",
    "\n",
    "### 1.1 Whitespace Tokenization\n",
    "Splits text based on whitespace characters (spaces, tabs, newlines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e81590a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace Tokenization:\n",
      "['Hello,', 'world!', 'This', 'is', 'a', 'sample', 'text', 'for', 'NLP.', 'It', 'contains', 'words', 'like', 'running,', 'flies,', 'better,', 'and', 'easily.']\n",
      "\n",
      "Number of tokens: 18\n"
     ]
    }
   ],
   "source": [
    "# Whitespace Tokenization\n",
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "whitespace_tokens = whitespace_tokenizer.tokenize(text)\n",
    "\n",
    "print(\"Whitespace Tokenization:\")\n",
    "print(whitespace_tokens)\n",
    "print(f\"\\nNumber of tokens: {len(whitespace_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339af486",
   "metadata": {},
   "source": [
    "### 1.2 Punctuation-based Tokenization\n",
    "Splits text on whitespace and punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22fd4af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation-based Tokenization:\n",
      "['Hello', ',', 'world', '!', 'This', 'is', 'a', 'sample', 'text', 'for', 'NLP', '.', 'It', 'contains', 'words', 'like', 'running', ',', 'flies', ',', 'better', ',', 'and', 'easily', '.']\n",
      "\n",
      "Number of tokens: 25\n"
     ]
    }
   ],
   "source": [
    "# Punctuation-based Tokenization\n",
    "wordpunct_tokenizer = WordPunctTokenizer()\n",
    "wordpunct_tokens = wordpunct_tokenizer.tokenize(text)\n",
    "\n",
    "print(\"Punctuation-based Tokenization:\")\n",
    "print(wordpunct_tokens)\n",
    "print(f\"\\nNumber of tokens: {len(wordpunct_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeec080",
   "metadata": {},
   "source": [
    "### 1.3 Treebank Tokenization\n",
    "Uses the conventions of the Penn Treebank for tokenization (separates contractions, handles punctuation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1eab657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treebank Tokenization:\n",
      "['Hello', ',', 'world', '!', 'This', 'is', 'a', 'sample', 'text', 'for', 'NLP.', 'It', 'contains', 'words', 'like', 'running', ',', 'flies', ',', 'better', ',', 'and', 'easily', '.']\n",
      "\n",
      "Number of tokens: 24\n"
     ]
    }
   ],
   "source": [
    "# Treebank Tokenization\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "treebank_tokens = treebank_tokenizer.tokenize(text)\n",
    "\n",
    "print(\"Treebank Tokenization:\")\n",
    "print(treebank_tokens)\n",
    "print(f\"\\nNumber of tokens: {len(treebank_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d929a21d",
   "metadata": {},
   "source": [
    "### 1.4 Tweet Tokenization\n",
    "Specially designed for tokenizing tweets, handles hashtags, mentions, emoticons, and URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e8ae9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Tokenization:\n",
      "['@NLPStudent', ':', 'Just', 'learned', 'about', '#NLP', 'and', '#MachineLearning', '!', 'ðŸ˜Š', 'Check', 'it', 'out', ':', 'https://example.com']\n",
      "\n",
      "Number of tokens: 15\n"
     ]
    }
   ],
   "source": [
    "# Tweet Tokenization\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = tweet_tokenizer.tokenize(tweet_text)\n",
    "\n",
    "print(\"Tweet Tokenization:\")\n",
    "print(tweet_tokens)\n",
    "print(f\"\\nNumber of tokens: {len(tweet_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1335e71c",
   "metadata": {},
   "source": [
    "### 1.5 MWE (Multi-Word Expression) Tokenization\n",
    "Treats specified multi-word expressions as single tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a013f049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "New York is a beautiful city. Machine learning is part of artificial intelligence.\n",
      "\n",
      "MWE Tokenization:\n",
      "['New_York', 'is', 'a', 'beautiful', 'city.', 'Machine', 'learning', 'is', 'part', 'of', 'artificial', 'intelligence.']\n",
      "\n",
      "Number of tokens: 12\n"
     ]
    }
   ],
   "source": [
    "# MWE Tokenization\n",
    "mwe_text = \"New York is a beautiful city. Machine learning is part of artificial intelligence.\"\n",
    "print(\"Original text:\")\n",
    "print(mwe_text)\n",
    "\n",
    "# First tokenize with a basic tokenizer\n",
    "basic_tokens = mwe_text.split()\n",
    "\n",
    "# Create MWE tokenizer and add multi-word expressions\n",
    "mwe_tokenizer = MWETokenizer([('New', 'York'), ('machine', 'learning'), ('artificial', 'intelligence')], separator='_')\n",
    "\n",
    "# Tokenize\n",
    "mwe_tokens = mwe_tokenizer.tokenize(basic_tokens)\n",
    "\n",
    "print(\"\\nMWE Tokenization:\")\n",
    "print(mwe_tokens)\n",
    "print(f\"\\nNumber of tokens: {len(mwe_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff418de4",
   "metadata": {},
   "source": [
    "## 2. Stemming\n",
    "\n",
    "Stemming reduces words to their root form by removing suffixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1462389a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words to be stemmed:\n",
      "['running', 'runs', 'ran', 'runner', 'easily', 'fairly', 'fairness', 'flying', 'flies', 'connection', 'connections', 'connected', 'connecting']\n"
     ]
    }
   ],
   "source": [
    "# Sample words for stemming\n",
    "words = ['running', 'runs', 'ran', 'runner', 'easily', 'fairly', 'fairness', \n",
    "         'flying', 'flies', 'connection', 'connections', 'connected', 'connecting']\n",
    "\n",
    "print(\"Words to be stemmed:\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c288e2ee",
   "metadata": {},
   "source": [
    "### 2.1 Porter Stemmer\n",
    "The Porter Stemmer is one of the most common stemming algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbe118f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer Results:\n",
      "--------------------------------------------------\n",
      "Original Word        Stemmed Word        \n",
      "--------------------------------------------------\n",
      "running              run                 \n",
      "runs                 run                 \n",
      "ran                  ran                 \n",
      "runner               runner              \n",
      "easily               easili              \n",
      "fairly               fairli              \n",
      "fairness             fair                \n",
      "flying               fli                 \n",
      "flies                fli                 \n",
      "connection           connect             \n",
      "connections          connect             \n",
      "connected            connect             \n",
      "connecting           connect             \n"
     ]
    }
   ],
   "source": [
    "# Porter Stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "print(\"Porter Stemmer Results:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Original Word':<20} {'Stemmed Word':<20}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for word in words:\n",
    "    stemmed = porter.stem(word)\n",
    "    print(f\"{word:<20} {stemmed:<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8738141b",
   "metadata": {},
   "source": [
    "### 2.2 Snowball Stemmer\n",
    "The Snowball Stemmer (Porter2) is an improved version of the Porter Stemmer and supports multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fff1c247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowball Stemmer Results:\n",
      "--------------------------------------------------\n",
      "Original Word        Stemmed Word        \n",
      "--------------------------------------------------\n",
      "running              run                 \n",
      "runs                 run                 \n",
      "ran                  ran                 \n",
      "runner               runner              \n",
      "easily               easili              \n",
      "fairly               fair                \n",
      "fairness             fair                \n",
      "flying               fli                 \n",
      "flies                fli                 \n",
      "connection           connect             \n",
      "connections          connect             \n",
      "connected            connect             \n",
      "connecting           connect             \n"
     ]
    }
   ],
   "source": [
    "# Snowball Stemmer\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "print(\"Snowball Stemmer Results:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Original Word':<20} {'Stemmed Word':<20}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for word in words:\n",
    "    stemmed = snowball.stem(word)\n",
    "    print(f\"{word:<20} {stemmed:<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f01e8e2",
   "metadata": {},
   "source": [
    "### 2.3 Comparison: Porter vs Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89791651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison: Porter vs Snowball Stemmer\n",
      "----------------------------------------------------------------------\n",
      "Original Word        Porter               Snowball            \n",
      "----------------------------------------------------------------------\n",
      "running              run                  run                 \n",
      "runs                 run                  run                 \n",
      "ran                  ran                  ran                 \n",
      "runner               runner               runner              \n",
      "easily               easili               easili              \n",
      "fairly               fairli               fair                \n",
      "fairness             fair                 fair                \n",
      "flying               fli                  fli                 \n",
      "flies                fli                  fli                 \n",
      "connection           connect              connect             \n",
      "connections          connect              connect             \n",
      "connected            connect              connect             \n",
      "connecting           connect              connect             \n"
     ]
    }
   ],
   "source": [
    "# Comparison of both stemmers\n",
    "print(\"Comparison: Porter vs Snowball Stemmer\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Original Word':<20} {'Porter':<20} {'Snowball':<20}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for word in words:\n",
    "    porter_stem = porter.stem(word)\n",
    "    snowball_stem = snowball.stem(word)\n",
    "    print(f\"{word:<20} {porter_stem:<20} {snowball_stem:<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5296e46",
   "metadata": {},
   "source": [
    "## 3. Lemmatization\n",
    "\n",
    "Lemmatization reduces words to their base or dictionary form (lemma) using vocabulary and morphological analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f18ef2",
   "metadata": {},
   "source": [
    "### 3.1 WordNet Lemmatizer\n",
    "Uses WordNet database to find the lemma of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4198cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet Lemmatizer Results (default - noun):\n",
      "--------------------------------------------------\n",
      "Original Word        Lemmatized Word     \n",
      "--------------------------------------------------\n",
      "running              running             \n",
      "runs                 run                 \n",
      "ran                  ran                 \n",
      "runner               runner              \n",
      "easily               easily              \n",
      "fairly               fairly              \n",
      "fairness             fairness            \n",
      "flying               flying              \n",
      "flies                fly                 \n",
      "connection           connection          \n",
      "connections          connection          \n",
      "connected            connected           \n",
      "connecting           connecting          \n"
     ]
    }
   ],
   "source": [
    "# WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"WordNet Lemmatizer Results (default - noun):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Original Word':<20} {'Lemmatized Word':<20}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for word in words:\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    print(f\"{word:<20} {lemma:<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a3c1a3",
   "metadata": {},
   "source": [
    "### 3.2 Lemmatization with POS Tags\n",
    "Lemmatization works better when we specify the Part of Speech (POS) of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b839eae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization with Different POS Tags:\n",
      "------------------------------------------------------------------------------------------\n",
      "Word            Noun            Verb            Adjective       Adverb         \n",
      "------------------------------------------------------------------------------------------\n",
      "running         running         run             running         running        \n",
      "runs            run             run             runs            runs           \n",
      "ran             ran             run             ran             ran            \n",
      "better          better          better          good            well           \n",
      "good            good            good            good            good           \n",
      "best            best            best            best            best           \n",
      "worse           worse           worse           bad             worse          \n",
      "worst           worst           worst           bad             worst          \n"
     ]
    }
   ],
   "source": [
    "# Lemmatization with different POS tags\n",
    "test_words = ['running', 'runs', 'ran', 'better', 'good', 'best', 'worse', 'worst']\n",
    "\n",
    "print(\"Lemmatization with Different POS Tags:\")\n",
    "print(\"-\" * 90)\n",
    "print(f\"{'Word':<15} {'Noun':<15} {'Verb':<15} {'Adjective':<15} {'Adverb':<15}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for word in test_words:\n",
    "    noun_lemma = lemmatizer.lemmatize(word, pos='n')\n",
    "    verb_lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "    adj_lemma = lemmatizer.lemmatize(word, pos='a')\n",
    "    adv_lemma = lemmatizer.lemmatize(word, pos='r')\n",
    "    print(f\"{word:<15} {noun_lemma:<15} {verb_lemma:<15} {adj_lemma:<15} {adv_lemma:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b582ebc8",
   "metadata": {},
   "source": [
    "## 4. Comparison: Stemming vs Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f42cd353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming vs Lemmatization Comparison:\n",
      "------------------------------------------------------------------------------------------\n",
      "Original        Porter          Snowball        Lemma (n)       Lemma (v)      \n",
      "------------------------------------------------------------------------------------------\n",
      "studies         studi           studi           study           study          \n",
      "studying        studi           studi           studying        study          \n",
      "cries           cri             cri             cry             cry            \n",
      "crying          cri             cri             cry             cry            \n",
      "better          better          better          better          better         \n",
      "caring          care            care            caring          care           \n",
      "leaves          leav            leav            leaf            leave          \n"
     ]
    }
   ],
   "source": [
    "# Comparison of Stemming vs Lemmatization\n",
    "comparison_words = ['studies', 'studying', 'cries', 'crying', 'better', 'caring', 'leaves']\n",
    "\n",
    "print(\"Stemming vs Lemmatization Comparison:\")\n",
    "print(\"-\" * 90)\n",
    "print(f\"{'Original':<15} {'Porter':<15} {'Snowball':<15} {'Lemma (n)':<15} {'Lemma (v)':<15}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for word in comparison_words:\n",
    "    porter_stem = porter.stem(word)\n",
    "    snowball_stem = snowball.stem(word)\n",
    "    noun_lemma = lemmatizer.lemmatize(word, pos='n')\n",
    "    verb_lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "    print(f\"{word:<15} {porter_stem:<15} {snowball_stem:<15} {noun_lemma:<15} {verb_lemma:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8d71a8",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "### Tokenization Methods:\n",
    "- **Whitespace**: Simple, splits only on spaces\n",
    "- **Punctuation-based**: Separates punctuation as individual tokens\n",
    "- **Treebank**: Standard linguistic conventions, handles contractions\n",
    "- **Tweet**: Specialized for social media content (hashtags, mentions, URLs, emojis)\n",
    "- **MWE**: Treats multi-word expressions as single tokens\n",
    "\n",
    "### Stemming vs Lemmatization:\n",
    "- **Stemming**: Faster, rule-based, may produce non-words (e.g., \"studi\" from \"studies\")\n",
    "- **Lemmatization**: Slower, dictionary-based, always produces valid words (e.g., \"study\" from \"studies\")\n",
    "- **Porter vs Snowball**: Snowball is generally more accurate and supports multiple languages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
