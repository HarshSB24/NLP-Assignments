{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c94d86e7",
   "metadata": {},
   "source": [
    "# NLP Assignment 3: Text Preprocessing and TF-IDF Representation\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Text cleaning\n",
    "- Lemmatization\n",
    "- Stop word removal\n",
    "- Label encoding\n",
    "- TF-IDF vectorization\n",
    "- Saving outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fbc5b8",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0560b849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bb2734",
   "metadata": {},
   "source": [
    "## 2. Create Sample Dataset\n",
    "\n",
    "Creating a sample dataset with text and labels for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a5f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset\n",
    "data = {\n",
    "    'text': [\n",
    "        \"This is a GREAT movie! I really loved it!!!\",\n",
    "        \"The product was terrible and didn't work at all...\",\n",
    "        \"Amazing experience! Would definitely recommend to friends.\",\n",
    "        \"Worst purchase ever. Complete waste of money!!!\",\n",
    "        \"It's okay, nothing special but not bad either.\",\n",
    "        \"Absolutely fantastic! Best thing I've ever bought!\",\n",
    "        \"Very disappointing. Expected much better quality.\",\n",
    "        \"Pretty good overall, happy with the purchase.\",\n",
    "        \"Not recommended!!! Poor quality and bad service.\",\n",
    "        \"Excellent product! Worth every penny. Highly satisfied!!!\"\n",
    "    ],\n",
    "    'label': ['positive', 'negative', 'positive', 'negative', 'neutral', \n",
    "              'positive', 'negative', 'positive', 'negative', 'positive']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "print(f\"\\nDataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5812d001",
   "metadata": {},
   "source": [
    "## 3. Text Cleaning\n",
    "\n",
    "Cleaning the text by:\n",
    "- Converting to lowercase\n",
    "- Removing special characters and punctuation\n",
    "- Removing extra whitespaces\n",
    "- Removing numbers (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132034ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean the input text\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "print(\"Text Cleaning Results:\")\n",
    "print(df[['text', 'cleaned_text']].head())\n",
    "print(f\"\\nSample cleaned text:\\n{df['cleaned_text'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d3fc8",
   "metadata": {},
   "source": [
    "## 4. Lemmatization\n",
    "\n",
    "Applying lemmatization to convert words to their base/dictionary form using NLTK's WordNetLemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e561c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    \"\"\"Lemmatize the input text\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply lemmatization\n",
    "df['lemmatized_text'] = df['cleaned_text'].apply(lemmatize_text)\n",
    "\n",
    "print(\"Lemmatization Results:\")\n",
    "print(df[['cleaned_text', 'lemmatized_text']].head())\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"Before: {df['cleaned_text'].iloc[0]}\")\n",
    "print(f\"After:  {df['lemmatized_text'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d413183b",
   "metadata": {},
   "source": [
    "## 5. Stop Words Removal\n",
    "\n",
    "Removing common English stop words using NLTK's stopwords corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d18dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stop words from text\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply stop words removal\n",
    "df['processed_text'] = df['lemmatized_text'].apply(remove_stopwords)\n",
    "\n",
    "print(\"Stop Words Removal Results:\")\n",
    "print(df[['lemmatized_text', 'processed_text']].head())\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"Before: {df['lemmatized_text'].iloc[0]}\")\n",
    "print(f\"After:  {df['processed_text'].iloc[0]}\")\n",
    "\n",
    "# Display the full preprocessing pipeline\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Complete Preprocessing Pipeline:\")\n",
    "print(\"=\"*80)\n",
    "print(df[['text', 'processed_text', 'label']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcd3538",
   "metadata": {},
   "source": [
    "## 6. Label Encoding\n",
    "\n",
    "Converting categorical labels to numerical values using sklearn's LabelEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0f6c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['encoded_label'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "print(\"Label Encoding Results:\")\n",
    "print(df[['label', 'encoded_label']].drop_duplicates().sort_values('encoded_label'))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Label Mapping:\")\n",
    "print(\"=\"*50)\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"{label}: {i}\")\n",
    "\n",
    "print(f\"\\nDataset with encoded labels:\")\n",
    "print(df[['text', 'processed_text', 'label', 'encoded_label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc95278c",
   "metadata": {},
   "source": [
    "## 7. TF-IDF Vectorization\n",
    "\n",
    "Creating TF-IDF (Term Frequency-Inverse Document Frequency) representations of the processed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b7f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the processed text\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['processed_text'])\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=tfidf_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "print(\"TF-IDF Matrix Shape:\", tfidf_matrix.shape)\n",
    "print(f\"Number of documents: {tfidf_matrix.shape[0]}\")\n",
    "print(f\"Number of features: {tfidf_matrix.shape[1]}\")\n",
    "\n",
    "print(\"\\nTF-IDF Feature Names (first 20):\")\n",
    "print(tfidf_vectorizer.get_feature_names_out()[:20])\n",
    "\n",
    "print(\"\\nTF-IDF Matrix (first 5 documents, first 10 features):\")\n",
    "print(tfidf_df.iloc[:5, :10])\n",
    "\n",
    "print(\"\\nFull TF-IDF DataFrame:\")\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52df393c",
   "metadata": {},
   "source": [
    "## 8. Visualization\n",
    "\n",
    "Analyzing the most important features based on TF-IDF scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92034f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average TF-IDF scores for each feature\n",
    "feature_scores = tfidf_df.mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 15 Features by Average TF-IDF Score:\")\n",
    "print(\"=\"*50)\n",
    "for feature, score in feature_scores.head(15).items():\n",
    "    print(f\"{feature:20s}: {score:.4f}\")\n",
    "\n",
    "# Create a summary of non-zero features per document\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Non-zero Features per Document:\")\n",
    "print(\"=\"*50)\n",
    "for idx in range(len(df)):\n",
    "    non_zero = (tfidf_df.iloc[idx] > 0).sum()\n",
    "    print(f\"Document {idx+1}: {non_zero} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5f338d",
   "metadata": {},
   "source": [
    "## 9. Save Outputs\n",
    "\n",
    "Saving all processed data, models, and results to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae476dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = 'outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Save processed dataset\n",
    "df.to_csv(f'{output_dir}/processed_data.csv', index=False)\n",
    "print(f\"✓ Saved processed data to '{output_dir}/processed_data.csv'\")\n",
    "\n",
    "# 2. Save TF-IDF matrix as CSV\n",
    "tfidf_df.to_csv(f'{output_dir}/tfidf_matrix.csv', index=False)\n",
    "print(f\"✓ Saved TF-IDF matrix to '{output_dir}/tfidf_matrix.csv'\")\n",
    "\n",
    "# 3. Save TF-IDF matrix as numpy array\n",
    "np.save(f'{output_dir}/tfidf_matrix.npy', tfidf_matrix.toarray())\n",
    "print(f\"✓ Saved TF-IDF matrix (numpy) to '{output_dir}/tfidf_matrix.npy'\")\n",
    "\n",
    "# 4. Save TF-IDF vectorizer\n",
    "with open(f'{output_dir}/tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "print(f\"✓ Saved TF-IDF vectorizer to '{output_dir}/tfidf_vectorizer.pkl'\")\n",
    "\n",
    "# 5. Save label encoder\n",
    "with open(f'{output_dir}/label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "print(f\"✓ Saved label encoder to '{output_dir}/label_encoder.pkl'\")\n",
    "\n",
    "# 6. Save feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "np.save(f'{output_dir}/feature_names.npy', feature_names)\n",
    "print(f\"✓ Saved feature names to '{output_dir}/feature_names.npy'\")\n",
    "\n",
    "# 7. Save label mapping\n",
    "label_mapping = {label: idx for idx, label in enumerate(label_encoder.classes_)}\n",
    "with open(f'{output_dir}/label_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(label_mapping, f)\n",
    "print(f\"✓ Saved label mapping to '{output_dir}/label_mapping.pkl'\")\n",
    "\n",
    "# 8. Create and save summary report\n",
    "summary_report = f\"\"\"\n",
    "NLP Assignment 3 - Processing Summary\n",
    "{'='*60}\n",
    "\n",
    "Dataset Information:\n",
    "- Total documents: {len(df)}\n",
    "- Unique labels: {df['label'].nunique()}\n",
    "- Labels: {', '.join(df['label'].unique())}\n",
    "\n",
    "TF-IDF Vectorization:\n",
    "- Total features: {tfidf_matrix.shape[1]}\n",
    "- Matrix shape: {tfidf_matrix.shape}\n",
    "- Non-zero elements: {tfidf_matrix.nnz}\n",
    "- Sparsity: {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.2f}%\n",
    "\n",
    "Label Encoding:\n",
    "{chr(10).join([f'- {label}: {idx}' for idx, label in enumerate(label_encoder.classes_)])}\n",
    "\n",
    "Top 10 Features by TF-IDF Score:\n",
    "{chr(10).join([f'- {feature}: {score:.4f}' for feature, score in feature_scores.head(10).items()])}\n",
    "\n",
    "Files Saved:\n",
    "- processed_data.csv: Full dataset with all preprocessing steps\n",
    "- tfidf_matrix.csv: TF-IDF matrix in CSV format\n",
    "- tfidf_matrix.npy: TF-IDF matrix in numpy format\n",
    "- tfidf_vectorizer.pkl: Trained TF-IDF vectorizer\n",
    "- label_encoder.pkl: Trained label encoder\n",
    "- feature_names.npy: List of TF-IDF features\n",
    "- label_mapping.pkl: Label to index mapping\n",
    "\"\"\"\n",
    "\n",
    "with open(f'{output_dir}/summary_report.txt', 'w') as f:\n",
    "    f.write(summary_report)\n",
    "print(f\"✓ Saved summary report to '{output_dir}/summary_report.txt'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All outputs saved successfully!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741e9c6a",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "This notebook completed the following tasks:\n",
    "\n",
    "1. **Text Cleaning**: Converted text to lowercase, removed special characters and extra whitespaces\n",
    "2. **Lemmatization**: Applied WordNetLemmatizer to convert words to their base form\n",
    "3. **Stop Words Removal**: Removed common English stop words using NLTK\n",
    "4. **Label Encoding**: Converted categorical labels to numerical values\n",
    "5. **TF-IDF Vectorization**: Created Term Frequency-Inverse Document Frequency representations\n",
    "6. **Output Saving**: Saved all processed data, models, and reports to the `outputs/` directory\n",
    "\n",
    "All outputs are saved and ready for further analysis or machine learning tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c32cd96",
   "metadata": {},
   "source": [
    "## Bonus: Loading Saved Models (Optional)\n",
    "\n",
    "Example of how to load and use the saved models for new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe17ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load saved models and process new text\n",
    "def process_new_text(new_text):\n",
    "    \"\"\"Process new text using saved models\"\"\"\n",
    "    # Load the vectorizer\n",
    "    with open(f'{output_dir}/tfidf_vectorizer.pkl', 'rb') as f:\n",
    "        loaded_vectorizer = pickle.load(f)\n",
    "    \n",
    "    # Load the label encoder\n",
    "    with open(f'{output_dir}/label_encoder.pkl', 'rb') as f:\n",
    "        loaded_encoder = pickle.load(f)\n",
    "    \n",
    "    # Process the new text\n",
    "    cleaned = clean_text(new_text)\n",
    "    lemmatized = lemmatize_text(cleaned)\n",
    "    processed = remove_stopwords(lemmatized)\n",
    "    \n",
    "    # Transform using TF-IDF\n",
    "    tfidf_vector = loaded_vectorizer.transform([processed])\n",
    "    \n",
    "    return processed, tfidf_vector\n",
    "\n",
    "# Test with new text\n",
    "new_text = \"This product is absolutely wonderful! I love it so much!!!\"\n",
    "processed, tfidf_vec = process_new_text(new_text)\n",
    "\n",
    "print(\"Example: Processing New Text\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original: {new_text}\")\n",
    "print(f\"Processed: {processed}\")\n",
    "print(f\"TF-IDF vector shape: {tfidf_vec.shape}\")\n",
    "print(f\"Non-zero features: {tfidf_vec.nnz}\")\n",
    "print(\"\\nTop 5 TF-IDF scores:\")\n",
    "feature_idx = tfidf_vec.toarray()[0].argsort()[-5:][::-1]\n",
    "with open(f'{output_dir}/tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    loaded_vectorizer = pickle.load(f)\n",
    "features = loaded_vectorizer.get_feature_names_out()\n",
    "for idx in feature_idx:\n",
    "    if tfidf_vec.toarray()[0][idx] > 0:\n",
    "        print(f\"  {features[idx]}: {tfidf_vec.toarray()[0][idx]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
