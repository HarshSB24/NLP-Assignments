{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94385c8c",
   "metadata": {},
   "source": [
    "# NLP Assignment 2: Text Vectorization Techniques\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Bag of Words (BoW)** - Count occurrence\n",
    "2. **Bag of Words (BoW)** - Normalized count occurrence\n",
    "3. **TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
    "4. **Word2Vec Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fea664",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "719ad96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Python\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.0) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e58f46d",
   "metadata": {},
   "source": [
    "## 2. Prepare Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab2be200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Documents:\n",
      "1. Natural language processing is a subfield of artificial intelligence\n",
      "2. Machine learning and deep learning are part of artificial intelligence\n",
      "3. Natural language processing uses machine learning algorithms\n",
      "4. Deep learning models are used in natural language processing\n",
      "5. Artificial intelligence includes machine learning and natural language processing\n"
     ]
    }
   ],
   "source": [
    "# Sample corpus of documents\n",
    "documents = [\n",
    "    \"Natural language processing is a subfield of artificial intelligence\",\n",
    "    \"Machine learning and deep learning are part of artificial intelligence\",\n",
    "    \"Natural language processing uses machine learning algorithms\",\n",
    "    \"Deep learning models are used in natural language processing\",\n",
    "    \"Artificial intelligence includes machine learning and natural language processing\"\n",
    "]\n",
    "\n",
    "print(\"Sample Documents:\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"{i}. {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860428aa",
   "metadata": {},
   "source": [
    "## 3. Bag of Words (BoW) - Count Occurrence\n",
    "\n",
    "The Bag of Words model represents text as a vector of word counts, ignoring grammar and word order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50431da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words (Count Occurrence):\n",
      "Vocabulary size: 20\n",
      "\n",
      "Vocabulary: ['algorithms', 'and', 'are', 'artificial', 'deep', 'in', 'includes', 'intelligence', 'is', 'language', 'learning', 'machine', 'models', 'natural', 'of', 'part', 'processing', 'subfield', 'used', 'uses']\n",
      "\n",
      "       algorithms  and  are  artificial  deep  in  includes  intelligence  is  \\\n",
      "Doc 1           0    0    0           1     0   0         0             1   1   \n",
      "Doc 2           0    1    1           1     1   0         0             1   0   \n",
      "Doc 3           1    0    0           0     0   0         0             0   0   \n",
      "Doc 4           0    0    1           0     1   1         0             0   0   \n",
      "Doc 5           0    1    0           1     0   0         1             1   0   \n",
      "\n",
      "       language  learning  machine  models  natural  of  part  processing  \\\n",
      "Doc 1         1         0        0       0        1   1     0           1   \n",
      "Doc 2         0         2        1       0        0   1     1           0   \n",
      "Doc 3         1         1        1       0        1   0     0           1   \n",
      "Doc 4         1         1        0       1        1   0     0           1   \n",
      "Doc 5         1         1        1       0        1   0     0           1   \n",
      "\n",
      "       subfield  used  uses  \n",
      "Doc 1         1     0     0  \n",
      "Doc 2         0     0     0  \n",
      "Doc 3         0     0     1  \n",
      "Doc 4         0     1     0  \n",
      "Doc 5         0     0     0  \n"
     ]
    }
   ],
   "source": [
    "# Create Count Vectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "bow_matrix = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (vocabulary)\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=feature_names)\n",
    "bow_df.index = [f\"Doc {i+1}\" for i in range(len(documents))]\n",
    "\n",
    "print(\"Bag of Words (Count Occurrence):\")\n",
    "print(f\"Vocabulary size: {len(feature_names)}\")\n",
    "print(f\"\\nVocabulary: {list(feature_names)}\\n\")\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a54614b",
   "metadata": {},
   "source": [
    "## 4. Bag of Words - Normalized Count Occurrence\n",
    "\n",
    "Normalized counts help compare documents of different lengths by dividing counts by the total number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81a24ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words (Normalized Count Occurrence):\n",
      "       algorithms     and     are  artificial    deep      in  includes  \\\n",
      "Doc 1      0.0000  0.0000  0.0000      0.1250  0.0000  0.0000    0.0000   \n",
      "Doc 2      0.0000  0.1000  0.1000      0.1000  0.1000  0.0000    0.0000   \n",
      "Doc 3      0.1429  0.0000  0.0000      0.0000  0.0000  0.0000    0.0000   \n",
      "Doc 4      0.0000  0.0000  0.1111      0.0000  0.1111  0.1111    0.0000   \n",
      "Doc 5      0.0000  0.1111  0.0000      0.1111  0.0000  0.0000    0.1111   \n",
      "\n",
      "       intelligence     is  language  learning  machine  models  natural  \\\n",
      "Doc 1        0.1250  0.125    0.1250    0.0000   0.0000  0.0000   0.1250   \n",
      "Doc 2        0.1000  0.000    0.0000    0.2000   0.1000  0.0000   0.0000   \n",
      "Doc 3        0.0000  0.000    0.1429    0.1429   0.1429  0.0000   0.1429   \n",
      "Doc 4        0.0000  0.000    0.1111    0.1111   0.0000  0.1111   0.1111   \n",
      "Doc 5        0.1111  0.000    0.1111    0.1111   0.1111  0.0000   0.1111   \n",
      "\n",
      "          of  part  processing  subfield    used    uses  \n",
      "Doc 1  0.125   0.0      0.1250     0.125  0.0000  0.0000  \n",
      "Doc 2  0.100   0.1      0.0000     0.000  0.0000  0.0000  \n",
      "Doc 3  0.000   0.0      0.1429     0.000  0.0000  0.1429  \n",
      "Doc 4  0.000   0.0      0.1111     0.000  0.1111  0.0000  \n",
      "Doc 5  0.000   0.0      0.1111     0.000  0.0000  0.0000  \n"
     ]
    }
   ],
   "source": [
    "# Normalize the count matrix (L1 normalization - divide by sum of all counts per document)\n",
    "bow_normalized = bow_matrix.toarray()\n",
    "row_sums = bow_normalized.sum(axis=1, keepdims=True)\n",
    "bow_normalized = bow_normalized / row_sums\n",
    "\n",
    "# Convert to DataFrame\n",
    "bow_normalized_df = pd.DataFrame(bow_normalized, columns=feature_names)\n",
    "bow_normalized_df.index = [f\"Doc {i+1}\" for i in range(len(documents))]\n",
    "\n",
    "print(\"Bag of Words (Normalized Count Occurrence):\")\n",
    "print(bow_normalized_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f8d36",
   "metadata": {},
   "source": [
    "## 5. TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "TF-IDF weighs words by their importance: frequent in a document but rare across all documents get higher scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "470e7521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "       algorithms     and     are  artificial    deep      in  includes  \\\n",
      "Doc 1      0.0000  0.0000  0.0000      0.3157  0.0000  0.0000    0.0000   \n",
      "Doc 2      0.0000  0.3235  0.3235      0.2686  0.3235  0.0000    0.0000   \n",
      "Doc 3      0.5186  0.0000  0.0000      0.0000  0.0000  0.0000    0.0000   \n",
      "Doc 4      0.0000  0.0000  0.3418      0.0000  0.3418  0.4237    0.0000   \n",
      "Doc 5      0.0000  0.3906  0.0000      0.3242  0.0000  0.0000    0.4842   \n",
      "\n",
      "       intelligence      is  language  learning  machine  models  natural  \\\n",
      "Doc 1        0.3157  0.4714    0.2656    0.0000   0.0000  0.0000   0.2656   \n",
      "Doc 2        0.2686  0.0000    0.0000    0.4518   0.2686  0.0000   0.0000   \n",
      "Doc 3        0.0000  0.0000    0.2922    0.2922   0.3473  0.0000   0.2922   \n",
      "Doc 4        0.0000  0.0000    0.2387    0.2387   0.0000  0.4237   0.2387   \n",
      "Doc 5        0.3242  0.0000    0.2728    0.2728   0.3242  0.0000   0.2728   \n",
      "\n",
      "           of   part  processing  subfield    used    uses  \n",
      "Doc 1  0.3803  0.000      0.2656    0.4714  0.0000  0.0000  \n",
      "Doc 2  0.3235  0.401      0.0000    0.0000  0.0000  0.0000  \n",
      "Doc 3  0.0000  0.000      0.2922    0.0000  0.0000  0.5186  \n",
      "Doc 4  0.0000  0.000      0.2387    0.0000  0.4237  0.0000  \n",
      "Doc 5  0.0000  0.000      0.2728    0.0000  0.0000  0.0000  \n"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_feature_names)\n",
    "tfidf_df.index = [f\"Doc {i+1}\" for i in range(len(documents))]\n",
    "\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "551933c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 important words per document (based on TF-IDF):\n",
      "============================================================\n",
      "\n",
      "Doc 1: Natural language processing is a subfield of artif...\n",
      "------------------------------------------------------------\n",
      "  is                  : 0.4714\n",
      "  subfield            : 0.4714\n",
      "  of                  : 0.3803\n",
      "  artificial          : 0.3157\n",
      "  intelligence        : 0.3157\n",
      "\n",
      "Doc 2: Machine learning and deep learning are part of art...\n",
      "------------------------------------------------------------\n",
      "  learning            : 0.4518\n",
      "  part                : 0.4010\n",
      "  and                 : 0.3235\n",
      "  are                 : 0.3235\n",
      "  deep                : 0.3235\n",
      "\n",
      "Doc 3: Natural language processing uses machine learning ...\n",
      "------------------------------------------------------------\n",
      "  algorithms          : 0.5186\n",
      "  uses                : 0.5186\n",
      "  machine             : 0.3473\n",
      "  language            : 0.2922\n",
      "  learning            : 0.2922\n",
      "\n",
      "Doc 4: Deep learning models are used in natural language ...\n",
      "------------------------------------------------------------\n",
      "  in                  : 0.4237\n",
      "  models              : 0.4237\n",
      "  used                : 0.4237\n",
      "  are                 : 0.3418\n",
      "  deep                : 0.3418\n",
      "\n",
      "Doc 5: Artificial intelligence includes machine learning ...\n",
      "------------------------------------------------------------\n",
      "  includes            : 0.4842\n",
      "  and                 : 0.3906\n",
      "  artificial          : 0.3242\n",
      "  intelligence        : 0.3242\n",
      "  machine             : 0.3242\n"
     ]
    }
   ],
   "source": [
    "# Analyze TF-IDF scores\n",
    "print(\"\\nTop 5 important words per document (based on TF-IDF):\")\n",
    "print(\"=\"*60)\n",
    "for i, doc_idx in enumerate(tfidf_df.index):\n",
    "    doc_scores = tfidf_df.loc[doc_idx]\n",
    "    top_words = doc_scores.nlargest(5)\n",
    "    print(f\"\\n{doc_idx}: {documents[i][:50]}...\")\n",
    "    print(\"-\" * 60)\n",
    "    for word, score in top_words.items():\n",
    "        if score > 0:\n",
    "            print(f\"  {word:20s}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a66efb",
   "metadata": {},
   "source": [
    "## 6. Word2Vec Embeddings\n",
    "\n",
    "Word2Vec creates dense vector representations of words that capture semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1036cfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Documents:\n",
      "Doc 1: ['natural', 'language', 'processing', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence']\n",
      "Doc 2: ['machine', 'learning', 'and', 'deep', 'learning', 'are', 'part', 'of', 'artificial', 'intelligence']\n",
      "Doc 3: ['natural', 'language', 'processing', 'uses', 'machine', 'learning', 'algorithms']\n",
      "Doc 4: ['deep', 'learning', 'models', 'are', 'used', 'in', 'natural', 'language', 'processing']\n",
      "Doc 5: ['artificial', 'intelligence', 'includes', 'machine', 'learning', 'and', 'natural', 'language', 'processing']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize documents for Word2Vec\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
    "\n",
    "print(\"Tokenized Documents:\")\n",
    "for i, tokens in enumerate(tokenized_docs, 1):\n",
    "    print(f\"Doc {i}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a46c84d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Model trained successfully!\n",
      "Vocabulary size: 21\n",
      "Vector dimensionality: 50\n"
     ]
    }
   ],
   "source": [
    "# Train Word2Vec model\n",
    "# Parameters:\n",
    "# - vector_size: dimensionality of word vectors\n",
    "# - window: maximum distance between current and predicted word\n",
    "# - min_count: ignores words with frequency less than this\n",
    "# - sg: 1 for skip-gram, 0 for CBOW\n",
    "\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_docs,\n",
    "    vector_size=50,\n",
    "    window=3,\n",
    "    min_count=1,\n",
    "    sg=1,  # Skip-gram\n",
    "    epochs=100\n",
    ")\n",
    "\n",
    "print(\"Word2Vec Model trained successfully!\")\n",
    "print(f\"Vocabulary size: {len(word2vec_model.wv)}\")\n",
    "print(f\"Vector dimensionality: {word2vec_model.wv.vector_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82fd3bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Embeddings (Word2Vec Vectors):\n",
      "============================================================\n",
      "\n",
      "'natural': [ 0.0146931  -0.01868898 -0.00132968  0.00742045 -0.00301441  0.01420391\n",
      "  0.0214204   0.01612241 -0.00467669  0.01201618]... (showing first 10 dimensions)\n",
      "Full vector shape: (50,)\n",
      "\n",
      "'language': [-0.01798476  0.00768935  0.00966274  0.01206527  0.01451019 -0.01418762\n",
      "  0.00465903  0.01443525 -0.0078618  -0.01484121]... (showing first 10 dimensions)\n",
      "Full vector shape: (50,)\n",
      "\n",
      "'processing': [-0.01713256  0.00932818 -0.00881199  0.0022169   0.01634271 -0.01073778\n",
      "  0.01165339 -0.01098105 -0.00942809  0.01617806]... (showing first 10 dimensions)\n",
      "Full vector shape: (50,)\n",
      "\n",
      "'machine': [-0.00070727  0.00644643 -0.01438979 -0.00216373  0.01459725  0.01254674\n",
      " -0.00417216  0.00822531 -0.01943616  0.00931134]... (showing first 10 dimensions)\n",
      "Full vector shape: (50,)\n",
      "\n",
      "'learning': [-0.00161282  0.00060139  0.00946599  0.01822652 -0.01933221 -0.01576339\n",
      "  0.01516495  0.02005878 -0.01218199 -0.00974327]... (showing first 10 dimensions)\n",
      "Full vector shape: (50,)\n",
      "\n",
      "'artificial': [-0.0176424   0.0192456  -0.0009313  -0.00319747  0.00886509 -0.01022734\n",
      "  0.00836705  0.01654806  0.00994939 -0.01781376]... (showing first 10 dimensions)\n",
      "Full vector shape: (50,)\n"
     ]
    }
   ],
   "source": [
    "# Display word vectors for some key words\n",
    "print(\"\\nWord Embeddings (Word2Vec Vectors):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "key_words = ['natural', 'language', 'processing', 'machine', 'learning', 'artificial']\n",
    "for word in key_words:\n",
    "    if word in word2vec_model.wv:\n",
    "        vector = word2vec_model.wv[word]\n",
    "        print(f\"\\n'{word}': {vector[:10]}... (showing first 10 dimensions)\")\n",
    "        print(f\"Full vector shape: {vector.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a8c9839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Word Similarity Analysis:\n",
      "============================================================\n",
      "\n",
      "Words most similar to 'machine':\n",
      "  language       : 0.2754\n",
      "  are            : 0.2705\n",
      "  used           : 0.2293\n",
      "\n",
      "Words most similar to 'natural':\n",
      "  in             : 0.2464\n",
      "  are            : 0.1646\n",
      "  artificial     : 0.1006\n",
      "\n",
      "Words most similar to 'artificial':\n",
      "  intelligence   : 0.2914\n",
      "  part           : 0.2819\n",
      "  are            : 0.2571\n"
     ]
    }
   ],
   "source": [
    "# Find similar words using Word2Vec\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Word Similarity Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_words = ['machine', 'natural', 'artificial']\n",
    "for word in test_words:\n",
    "    if word in word2vec_model.wv:\n",
    "        print(f\"\\nWords most similar to '{word}':\")\n",
    "        similar_words = word2vec_model.wv.most_similar(word, topn=3)\n",
    "        for similar_word, similarity in similar_words:\n",
    "            print(f\"  {similar_word:15s}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fc5bc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document Embeddings (Word2Vec - averaged word vectors):\n",
      "============================================================\n",
      "Doc 1 embedding shape: (50,)\n",
      "  First 10 dimensions: [-5.0833886e-03  1.8785053e-03 -2.4312432e-03  3.3965025e-05\n",
      " -1.6989702e-03 -9.4283614e-03  3.9328886e-03  6.4147082e-03\n",
      " -5.4802126e-03 -6.6288989e-03]\n",
      "\n",
      "Doc 2 embedding shape: (50,)\n",
      "  First 10 dimensions: [-0.00378224  0.00388806 -0.00300068 -0.00431014 -0.00550977 -0.00615963\n",
      "  0.00437296  0.00823436 -0.00594596 -0.00830627]\n",
      "\n",
      "Doc 3 embedding shape: (50,)\n",
      "  First 10 dimensions: [-0.00853967  0.00293612  0.0030956   0.00590068  0.00358409  0.00133784\n",
      "  0.00738684  0.00685054 -0.00959731  0.00473232]\n",
      "\n",
      "Doc 4 embedding shape: (50,)\n",
      "  First 10 dimensions: [ 0.00022861 -0.00370952 -0.0031969   0.00464718 -0.00277108 -0.00417245\n",
      "  0.00935719  0.00510561 -0.00798658 -0.00360427]\n",
      "\n",
      "Doc 5 embedding shape: (50,)\n",
      "  First 10 dimensions: [-0.00284712  0.00132368  0.00156641  0.00317542 -0.00110207 -0.0055602\n",
      "  0.00660508  0.0030253  -0.00862447 -0.0051529 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create document embeddings by averaging word vectors\n",
    "def get_document_embedding(tokens, model):\n",
    "    \"\"\"Average word vectors to get document embedding\"\"\"\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.wv.vector_size)\n",
    "\n",
    "# Generate embeddings for all documents\n",
    "doc_embeddings = [get_document_embedding(tokens, word2vec_model) for tokens in tokenized_docs]\n",
    "\n",
    "print(\"\\nDocument Embeddings (Word2Vec - averaged word vectors):\")\n",
    "print(\"=\"*60)\n",
    "for i, embedding in enumerate(doc_embeddings, 1):\n",
    "    print(f\"Doc {i} embedding shape: {embedding.shape}\")\n",
    "    print(f\"  First 10 dimensions: {embedding[:10]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf1e24c",
   "metadata": {},
   "source": [
    "## 7. Summary and Comparison\n",
    "\n",
    "### Comparison of Methods:\n",
    "\n",
    "| Method | Type | Dimensionality | Semantic Meaning | Sparsity |\n",
    "|--------|------|----------------|------------------|----------|\n",
    "| **BoW (Count)** | Frequency-based | Vocabulary size | No | High (sparse) |\n",
    "| **BoW (Normalized)** | Frequency-based | Vocabulary size | No | High (sparse) |\n",
    "| **TF-IDF** | Weighted frequency | Vocabulary size | No | High (sparse) |\n",
    "| **Word2Vec** | Neural embedding | Fixed (50 in this example) | Yes | Low (dense) |\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "1. **BoW & TF-IDF**: Create sparse vectors where each dimension represents a word in vocabulary\n",
    "2. **Word2Vec**: Creates dense vectors that capture semantic relationships between words\n",
    "3. **Normalization**: Helps handle documents of varying lengths\n",
    "4. **TF-IDF**: Reduces weight of common words across documents\n",
    "5. **Word2Vec**: Can find similar words and perform word arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40bf101a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality Comparison:\n",
      "============================================================\n",
      "BoW (Count):         20 dimensions (vocabulary size)\n",
      "BoW (Normalized):    20 dimensions (vocabulary size)\n",
      "TF-IDF:              20 dimensions (vocabulary size)\n",
      "Word2Vec:            50 dimensions (fixed)\n",
      "\n",
      "Number of documents: 5\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Final comparison\n",
    "print(\"Dimensionality Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"BoW (Count):         {bow_matrix.shape[1]} dimensions (vocabulary size)\")\n",
    "print(f\"BoW (Normalized):    {bow_normalized.shape[1]} dimensions (vocabulary size)\")\n",
    "print(f\"TF-IDF:              {tfidf_matrix.shape[1]} dimensions (vocabulary size)\")\n",
    "print(f\"Word2Vec:            {word2vec_model.wv.vector_size} dimensions (fixed)\")\n",
    "print()\n",
    "print(f\"Number of documents: {len(documents)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc1ed5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
